{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71396607-531f-4f3d-98d4-36e195f4275f",
   "metadata": {},
   "source": [
    "# 터미널에서 실행할 것\n",
    "\n",
    "## elasticsearch 설치\n",
    "```bash\n",
    "wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.12.1-linux-x86_64.tar.gz\n",
    "wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.12.1-linux-x86_64.tar.gz.sha512\n",
    "shasum -a 512 -c elasticsearch-7.12.1-linux-x86_64.tar.gz.sha512 \n",
    "tar -xzf elasticsearch-7.12.1-linux-x86_64.tar.gz\n",
    "```\n",
    "\n",
    "## haystack 설치\n",
    "`pip install farm-haystack`\n",
    "\n",
    "## 한국어 형태소 분석기 설치\n",
    "```bash\n",
    "cd elasticsearch-7.12.1\n",
    "elasticsearch-7.12.1/elasticsearch-plugin install analysis-nori;\n",
    "\n",
    "chown -R daemon:daemon elasticsearch-7.12.1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2318ac-031c-4f3b-b33c-cbc7ecaca360",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModelForSeq2SeqLM, AutoTokenizer, Adafactor\n",
    "from datasets import load_metric, load_from_disk, load_dataset, Dataset, DatasetDict\n",
    "import torch\n",
    "import nltk\n",
    "model_name = 'KETI-AIR/ke-t5-large-newslike'\n",
    "metric = load_metric('squad')\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    use_fast=True,\n",
    "    #cache_dir=None,\n",
    ")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_name,\n",
    "    config=config,\n",
    "    cache_dir=None,\n",
    ")\n",
    "\n",
    "model.load_state_dict(torch.load('/opt/ml/p3-mrc-dok/t5_train_pred/KETI-AIR/ke-t5-large/outputs/pytorch_model.bin'))\n",
    "max_source_length = 1040\n",
    "max_target_length = 40\n",
    "padding = False\n",
    "\n",
    "model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0de583-25cf-4156-a5df-a5e42346cd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "def normalize_answer(s):\n",
    "    def remove_(text):\n",
    "        \"\"\" 불필요한 기호 제거 \"\"\"\n",
    "        text = re.sub(\"'\", \" \", text)\n",
    "        text = re.sub('\"', \" \", text)\n",
    "        text = re.sub(\"《\", \" \", text)\n",
    "        text = re.sub(\"》\", \" \", text)\n",
    "        text = re.sub(\"<\", \" \", text)\n",
    "        text = re.sub(\">\", \" \", text)\n",
    "        text = re.sub(\"〈\", \" \", text)\n",
    "        text = re.sub(\"〉\", \" \", text)\n",
    "        text = re.sub(\"\\(\", \" \", text)\n",
    "        text = re.sub(\"\\)\", \" \", text)\n",
    "        text = re.sub(\"‘\", \" \", text)\n",
    "        text = re.sub(\"’\", \" \", text)\n",
    "        return text\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_punc(lower(remove_(s))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e154c599-dbed-4b88-9c59-292a0d5f20c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = load_from_disk('/opt/ml/input/data/data/test_dataset/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca6b9bf-809c-40d8-bc01-09ed993aa646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "es_server = Popen(['/opt/ml/elasticsearch-7.12.1/bin/elasticsearch'],\n",
    "                   stdout=PIPE, stderr=STDOUT,\n",
    "                   preexec_fn=lambda: os.setuid(1)  # as daemon\n",
    "                  )\n",
    "# wait until ES has started\n",
    "! sleep 30\n",
    "\n",
    "mapping = {\n",
    "                      'settings':{\n",
    "                          'analysis':{\n",
    "                              'analyzer':{\n",
    "                                  'my_analyzer':{\n",
    "                                      \"type\": \"custom\",\n",
    "                                      'tokenizer':'nori_tokenizer',\n",
    "                                      'decompound_mode':'mixed',\n",
    "                                      'stopwords':'_korean_',\n",
    "                                      \"filter\": [\"lowercase\",\n",
    "                                                 \"my_shingle_f\",\n",
    "                                                 \"nori_readingform\",\n",
    "                                                 \"nori_number\"]\n",
    "                                  }\n",
    "                              },\n",
    "                              'filter':{\n",
    "                                  'my_shingle_f':{\n",
    "                                      \"type\": \"shingle\"\n",
    "                                  }\n",
    "                              }\n",
    "                          },\n",
    "                          'similarity':{\n",
    "                              'my_similarity':{\n",
    "                                  'type':'BM25',\n",
    "                              }\n",
    "                          }\n",
    "                      },\n",
    "                      'mappings':{\n",
    "                          'properties':{\n",
    "                              'title':{\n",
    "                                  'type':'text',\n",
    "                                  'analyzer':'my_analyzer',\n",
    "                                  'similarity':'my_similarity'\n",
    "                              },\n",
    "                              'text':{\n",
    "                                  'type':'text',\n",
    "                                  'analyzer':'my_analyzer',\n",
    "                                  'similarity':'my_similarity'\n",
    "                              }\n",
    "                          }\n",
    "                      }\n",
    "                  }\n",
    "\n",
    "from haystack.document_store import ElasticsearchDocumentStore\n",
    "document_store = ElasticsearchDocumentStore(host=\"localhost\", username=\"\", password=\"\", index=\"document\", custom_mapping=mapping, timeout=60)\n",
    "\n",
    "with open('/opt/ml/input/data/data/wikipedia_documents.json', \"r\") as f:\n",
    "    wiki = json.load(f)\n",
    "contexts = list(dict.fromkeys([v['text'] for v in wiki.values()]))\n",
    "\n",
    "\n",
    "dicts = [\n",
    "    {\n",
    "        'text': context,\n",
    "        'meta': {}\n",
    "    } for context in tqdm(contexts)\n",
    "]\n",
    "document_store.write_documents(dicts)\n",
    "\n",
    "from haystack.retriever import ElasticsearchRetriever\n",
    "retriever = ElasticsearchRetriever(document_store)\n",
    "from haystack.pipeline import DocumentSearchPipeline\n",
    "pipe = DocumentSearchPipeline(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e434f80-44d2-4eee-8d4e-66566a910eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "topk = 5\n",
    "submit = {}\n",
    "\n",
    "for t in test_dataset['validation']:\n",
    "    b = pipe.run(t['question'], top_k_retriever=topk)\n",
    "    #print(b)\n",
    "    prop = []\n",
    "    pred_list = []\n",
    "    docs = []\n",
    "    doc_score = []\n",
    "    doc_prop = []\n",
    "    for context in b['documents']:\n",
    "        question = re.sub(r'\\\\n+|날짜=[\\d]+-[\\d]+-[\\d]+', ' ', t['question']).strip()\n",
    "        question = re.sub(r'\\([一-龥]+\\)', '', question)\n",
    "        p = re.sub(r'\\\\n+|날짜=[\\d]+-[\\d]+-[\\d]+', ' ', context['text']).strip()\n",
    "        p = re.sub(r'\\([一-龥]+\\)', '', p)\n",
    "        tok = tokenizer(\n",
    "            f\"question : {question} context : \",\n",
    "            f\"{p}\",\n",
    "            truncation = 'only_second',\n",
    "            max_length = 1041,\n",
    "            stride = 300,\n",
    "            return_overflowing_tokens=True,\n",
    "            return_offsets_mapping=True,\n",
    "            padding=False,\n",
    "        )\n",
    "        \n",
    "        for inputs in tok['input_ids']:\n",
    "            eos_index = inputs.index(1)\n",
    "            k = inputs.pop(eos_index)\n",
    "            \n",
    "        for input_ids in tok['input_ids']:\n",
    "            outputs = model.generate(torch.tensor([input_ids]).to('cuda'), num_beams=5, num_return_sequences=5, return_dict_in_generate=True, output_scores=True)\n",
    "            pred = normalize_answer(tokenizer.decode(list(outputs.values())[0][0], skip_special_tokens=True).strip())\n",
    "            pred_list.append(pred)\n",
    "            prop.append(outputs.sequences_scores[0].item() + math.log(context['probability']))\n",
    "            docs.append(tokenizer.decode(input_ids))\n",
    "            doc_score.append(context['score'])\n",
    "            doc_prop.append(context['probability'])\n",
    "    \n",
    "    submit[t['id']] = pred_list[np.argmax(prop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f369da-755f-44ba-b3b8-3d5534a9f114",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pred_t5_inference.json\", \"w\") as writer:\n",
    "    writer.write(json.dumps(submit, indent=4, ensure_ascii=False) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
