{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c2318ac-031c-4f3b-b33c-cbc7ecaca360",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModelForSeq2SeqLM, AutoTokenizer, Adafactor\n",
    "from datasets import load_metric, load_from_disk, load_dataset, Dataset, DatasetDict\n",
    "import torch\n",
    "import nltk\n",
    "model_name = 'KETI-AIR/ke-t5-large-newslike'\n",
    "metric = load_metric('squad')\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    use_fast=True,\n",
    "    #cache_dir=None,\n",
    ")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_name,\n",
    "    config=config,\n",
    "    cache_dir=None,\n",
    ")\n",
    "\n",
    "model.load_state_dict(torch.load('/opt/ml/code/models/train_dataset/pytorch_model.bin'))\n",
    "\n",
    "model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d9c10c6-7511-4027-a03c-7e642f7d75e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "from haystack.document_store import ElasticsearchDocumentStore\n",
    "from haystack.retriever import ElasticsearchRetriever\n",
    "from haystack.pipeline import DocumentSearchPipeline\n",
    "import json\n",
    "\n",
    "es_server = Popen(['/opt/ml/elasticsearch-7.12.1/bin/elasticsearch'],\n",
    "                   stdout=PIPE, stderr=STDOUT,\n",
    "                   preexec_fn=lambda: os.setuid(1)  # as daemon\n",
    "                  )\n",
    "# wait until ES has started\n",
    "! sleep 30\n",
    "\n",
    "mapping = {\n",
    "                      'settings':{\n",
    "                          'analysis':{\n",
    "                              'analyzer':{\n",
    "                                  'my_analyzer':{\n",
    "                                      \"type\": \"custom\",\n",
    "                                      'tokenizer':'nori_tokenizer',\n",
    "                                      'decompound_mode':'mixed',\n",
    "                                      'stopwords':'_korean_',\n",
    "                                      \"filter\": [\"lowercase\",\n",
    "                                                 \"my_shingle_f\",\n",
    "                                                 \"nori_readingform\",\n",
    "                                                 \"nori_number\"]\n",
    "                                  }\n",
    "                              },\n",
    "                              'filter':{\n",
    "                                  'my_shingle_f':{\n",
    "                                      \"type\": \"shingle\"\n",
    "                                  }\n",
    "                              }\n",
    "                          },\n",
    "                          'similarity':{\n",
    "                              'my_similarity':{\n",
    "                                  'type':'BM25',\n",
    "                              }\n",
    "                          }\n",
    "                      },\n",
    "                      'mappings':{\n",
    "                          'properties':{\n",
    "                              'title':{\n",
    "                                  'type':'text',\n",
    "                                  'analyzer':'my_analyzer',\n",
    "                                  'similarity':'my_similarity'\n",
    "                              },\n",
    "                              'text':{\n",
    "                                  'type':'text',\n",
    "                                  'analyzer':'my_analyzer',\n",
    "                                  'similarity':'my_similarity'\n",
    "                              }\n",
    "                          }\n",
    "                      }\n",
    "                  }\n",
    "\n",
    "\n",
    "# from haystack.document_store.elasticsearch import ElasticsearchDocumentStore\n",
    "document_store = ElasticsearchDocumentStore(host=\"localhost\", username=\"\", password=\"\", index=\"document\", custom_mapping=mapping)\n",
    "\n",
    "if len(document_store.get_all_documents()) == 0:\n",
    "    with open('/opt/ml/input/data/data/wikipedia_documents.json', \"r\") as f:\n",
    "        wiki = json.load(f)\n",
    "    contexts = list(dict.fromkeys([v['text'] for v in wiki.values()]))\n",
    "\n",
    "    dicts = [\n",
    "        {\n",
    "            'text': context,\n",
    "            'meta': {}\n",
    "        } for context in contexts\n",
    "    ]\n",
    "    document_store.write_documents(dicts)\n",
    "\n",
    "retriever = ElasticsearchRetriever(document_store)\n",
    "pipe = DocumentSearchPipeline(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f0de583-25cf-4156-a5df-a5e42346cd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "def normalize_answer(s):\n",
    "    def remove_(text):\n",
    "        \"\"\" 불필요한 기호 제거 \"\"\"\n",
    "        text = re.sub(\"'\", \" \", text)\n",
    "        text = re.sub('\"', \" \", text)\n",
    "        text = re.sub(\"《\", \" \", text)\n",
    "        text = re.sub(\"》\", \" \", text)\n",
    "        text = re.sub(\"<\", \" \", text)\n",
    "        text = re.sub(\">\", \" \", text)\n",
    "        text = re.sub(\"〈\", \" \", text)\n",
    "        text = re.sub(\"〉\", \" \", text)\n",
    "        text = re.sub(\"\\(\", \" \", text)\n",
    "        text = re.sub(\"\\)\", \" \", text)\n",
    "        text = re.sub(\"‘\", \" \", text)\n",
    "        text = re.sub(\"’\", \" \", text)\n",
    "        return text\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_punc(lower(remove_(s))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e154c599-dbed-4b88-9c59-292a0d5f20c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = load_from_disk('/opt/ml/input/data/data/test_dataset/')['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03ebe3ce-ca92-428a-a193-87526c114199",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "submit = {}\n",
    "submit_detail = {}\n",
    "topk = 1\n",
    "\n",
    "for t in tqdm(test_dataset):\n",
    "    b = pipe.run(t['question'], top_k_retriever=topk)\n",
    "    pred_list = []\n",
    "    prop = []\n",
    "    docs = []\n",
    "    doc_score = []\n",
    "    doc_prop = []\n",
    "    for idx, context in enumerate(b['documents']):\n",
    "        question = re.sub(r'\\\\n+|날짜=[\\d]+-[\\d]+-[\\d]+', ' ', t['question']).strip()\n",
    "        question = re.sub(r'\\([一-龥]+\\)', '', question)\n",
    "        p = re.sub(r'\\\\n+|날짜=[\\d]+-[\\d]+-[\\d]+', ' ', context['text']).strip()\n",
    "        p = re.sub(r'\\([一-龥]+\\)', '', p)\n",
    "\n",
    "        tok = tokenizer(\n",
    "            f\"question : {question} context : \",\n",
    "            f\"{p}\",\n",
    "            truncation = 'only_second',\n",
    "            max_length = 1041,\n",
    "            stride = 300,\n",
    "            return_overflowing_tokens=True,\n",
    "            return_offsets_mapping=True,\n",
    "            padding=False,\n",
    "        )\n",
    "\n",
    "        for inputs in tok['input_ids']:\n",
    "            eos_index = inputs.index(1)\n",
    "            k = inputs.pop(eos_index)\n",
    "\n",
    "        for input_ids in tok['input_ids']:\n",
    "            outputs = model.generate(torch.tensor([input_ids]).to('cuda'), num_beams=5, num_return_sequences=5, return_dict_in_generate=True, output_scores=True)\n",
    "            pred = normalize_answer(tokenizer.decode(list(outputs.values())[0][0], skip_special_tokens=True).strip())\n",
    "            pred_list.append(pred)\n",
    "            prop.append((outputs.sequences_scores[0]))\n",
    "            docs.append(tokenizer.decode(input_ids))\n",
    "            doc_score.append(context['score'])\n",
    "            doc_prop.append(context['probability'])\n",
    "\n",
    "    submit[t['id']] = pred_list[np.argmax(prop)]\n",
    "    submit_detail[t['id']] = {'doc_score':doc_score, 'docs':docs, 'doc_prop':doc_prop, 'prop':prop, 'pred_list':pred_list}\n",
    "    \n",
    "output_path = '/opt/ml/code/outputs/test_dataset/'\n",
    "pred_path = os.path.join(output_path, 'predictions.json')\n",
    "pred_info_path = os.path.join(output_path, 'pred_info')\n",
    "\n",
    "if not os.path.isdir(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "with open(pred_path, \"w\") as writer:\n",
    "    writer.write(json.dumps(submit, indent=4, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "with open(pred_info_path, \"wb\") as file:\n",
    "    pickle.dump(submit_detail, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
