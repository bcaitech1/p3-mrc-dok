bert-base-multilingual-cased:
  model_name: "bert-base-multilingual-cased"
  seed: 0
  train_args:
    retrieval: 1
    num_epochs: 15
    train_batch_size: 8
    eval_batch_size: 8
    weight_decay: 0.01
    gradient_accumulation_steps: 1
    optimizer: 'AdamW'
    adam_epsilon: 1.0e-8
    criterion: 'label_smoothing' # (focal, label_smoothing, f1, cross_entropy)
    smoothing: 0.2
    scheduler_name: 'ReduceLROnPlateau' # ['CosineAnnealing', 'ReduceLROnPlateau', 'steplr', 'linear']
    lr: 0.00001
    gamma: 0.5
    warmup_epoch: 1
    first_cycle_epoch: 3 # if scheduler == CosineAnnealing
    min_lr : 0.0000001 # if scheduler == Cosine Anneaing
  model_args:
    max_seq_length: 128
    pad_to_max_length: 0 # "Whether to pad all samples to `max_seq_length`. ""If False, will pad the samples dynamically when batching to the maximum length in the batch (which can be faster on GPU but will be slower on TPU)."
    doc_stride: 50 # When splitting up a long document into chunks, how much stride to take between chunks.
    model_save_dir: '/opt/ml/my_code/output/model_saved'
  post_process:
    n_best_size: 20
    max_answer_length: 30
  dataset:
    train_dataset: "/opt/ml/input/data/data/train_dataset"
    test_dataset: '/opt/ml/input/data/data/test_dataset'
    preprocessing_num_workers: 1
  inference:
    retrieval: 1
    data_path: /opt/ml/input/data/data
    context_path: wikipedia_documents.json
    output_dir: /opt/ml/my_code/output/submission


xlm-roberta-large:
  model_name: "xlm-roberta-large"
  seed: 0
  train_args:
    retrieval: 1
    num_epochs: 15
    train_batch_size: 8
    eval_batch_size: 8
    weight_decay: 0.01
    gradient_accumulation_steps: 1
    optimizer: 'AdamW'
    adam_epsilon: 1.0e-8
    criterion: 'label_smoothing' # (focal, label_smoothing, f1, cross_entropy)
    smoothing: 0.2
    scheduler_name: 'ReduceLROnPlateau' # ['CosineAnnealing', 'ReduceLROnPlateau', 'steplr', 'linear']
    lr: 0.00001
    gamma: 0.1
    warmup_epoch: 3
    first_cycle_epoch: 6 # if scheduler == CosineAnnealing
    min_lr : 0.00001 # if scheduler == Cosine Anneaing
  model_args:
    max_seq_length: 128
    pad_to_max_length: 0 # "Whether to pad all samples to `max_seq_length`. ""If False, will pad the samples dynamically when batching to the maximum length in the batch (which can be faster on GPU but will be slower on TPU)."
    doc_stride: 50 # When splitting up a long document into chunks, how much stride to take between chunks.
    model_save_dir: '/opt/ml/my_code/output/model_saved'
  post_process:
    n_best_size: 20
    max_answer_length: 30
  dataset:
    train_dataset: "/opt/ml/input/data/data/train_dataset"
    test_dataset: '/opt/ml/input/data/data/test_dataset'
    preprocessing_num_workers: 1
  inference:
    retrieval: 1
    data_path: /opt/ml/input/data/data
    context_path: wikipedia_documents.json
    output_dir: /opt/ml/my_code/output/submission

