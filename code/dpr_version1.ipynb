{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96a8a3bb-2b10-4f5f-8ee2-297df121d8c5",
   "metadata": {},
   "source": [
    "## 0. install , import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc40c3a9-f42e-4205-9db1-03fd888e878f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "#!pip install transformers\n",
    "!pip install git+https://github.com/huggingface/transformers\n",
    "!pip install wandb --upgrade\n",
    "!wandb login\n",
    "import wandb\n",
    "import os\n",
    "wandb.init(project=\"dpr\")\n",
    "os.environ['WANDB_LOG_MODEL'] = 'true' #false by default\n",
    "os.environ['WANDB_WATCH'] = 'all'\n",
    "\n",
    "\n",
    "from datasets import load_dataset, load_from_disk, concatenate_datasets\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm,trange\n",
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel, BertPreTrainedModel, AdamW, TrainingArguments, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import (DataLoader, RandomSampler, TensorDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ea4e1c-e6b8-4be9-908b-94f3b6af5537",
   "metadata": {},
   "source": [
    "## 1.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0af7c9-63b5-4083-8edb-64cb47018b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"bert-base-multilingual-cased\"\n",
    "seed = 2021\n",
    "\n",
    "\n",
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "class BertEncoder(BertPreTrainedModel):\n",
    "  def __init__(self, config):\n",
    "    super(BertEncoder, self).__init__(config)\n",
    "\n",
    "    self.bert = BertModel(config)\n",
    "    self.init_weights()\n",
    "      \n",
    "  def forward(self, input_ids, \n",
    "              attention_mask=None, token_type_ids=None): \n",
    "  \n",
    "      outputs = self.bert(input_ids,\n",
    "                          attention_mask=attention_mask,\n",
    "                          token_type_ids=token_type_ids)\n",
    "      \n",
    "      pooled_output = outputs[1]\n",
    "\n",
    "      return pooled_output\n",
    "\n",
    "seed_everything(seed)\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"/opt/ml/input/dpr\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    dataloader_drop_last  = True,\n",
    "    dataloader_num_workers  = 4, \n",
    "    seed = seed,\n",
    "    gradient_accumulation_steps  = 2,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e57007c-7327-4596-aa91-141347802b93",
   "metadata": {},
   "source": [
    "## 2. dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f3af96-edf4-40c5-8a5c-be286e23fe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"squad_kor_v1\")\n",
    "mrc_dataset = load_from_disk('/opt/ml/input/data/data/train_dataset')\n",
    "mrc_dataset = mrc_dataset.remove_columns(['__index_level_0__', 'document_id'])\n",
    "mrc_dataset_train = mrc_dataset['train'].map(features=dataset['train'].features) #, keep_in_memory=True\n",
    "mrc_dataset_validation = mrc_dataset['validation'].map(features=dataset['validation'].features) #, keep_in_memory=True\n",
    "dataset['train'] = concatenate_datasets([mrc_dataset_train, dataset['train']])\n",
    "dataset['validation'] = concatenate_datasets([mrc_dataset_validation, dataset['validation']])\n",
    "\n",
    "\n",
    "training_dataset = dataset['train']\n",
    "q_seqs = tokenizer(training_dataset['question'], padding=\"max_length\", truncation=True, return_tensors='pt')\n",
    "p_seqs = tokenizer(training_dataset['context'], padding=\"max_length\", truncation=True, return_tensors='pt')\n",
    "train_dataset = TensorDataset(p_seqs['input_ids'], p_seqs['attention_mask'], p_seqs['token_type_ids'], \n",
    "                        q_seqs['input_ids'], q_seqs['attention_mask'], q_seqs['token_type_ids'])\n",
    "\n",
    "\n",
    "validate_dataset = dataset['validation']\n",
    "v_q_seqs = tokenizer(validate_dataset['question'], padding='max_length', truncation=True, return_tensors='pt')\n",
    "v_p_seqs = tokenizer(validate_dataset['context'], padding='max_length', truncation=True, return_tensors='pt')\n",
    "valid_dataset = TensorDataset(v_p_seqs['input_ids'], v_p_seqs['attention_mask'], v_p_seqs['token_type_ids'],\n",
    "                              v_q_seqs['input_ids'], v_q_seqs['attention_mask'], v_q_seqs['token_type_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703bffdd-4aa1-48a3-ad6d-2aed480a6e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2311a750-296c-4a58-938b-d8428878fa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3011a710-8a45-407b-bd6d-fd03bababe53",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_encoder = BertEncoder.from_pretrained(model_checkpoint)\n",
    "q_encoder = BertEncoder.from_pretrained(model_checkpoint)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    p_encoder.cuda()\n",
    "    q_encoder.cuda()\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eb44a7-9583-400d-8c53-fa3cd52059cf",
   "metadata": {},
   "source": [
    "## 3. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b54b1d-a0b8-480b-9783-a271647f3529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset,  batch_size=args.per_device_train_batch_size, drop_last = True,shuffle=True)\n",
    "\n",
    "valid_sampler = RandomSampler(valid_dataset)\n",
    "valid_loader = DataLoader(valid_dataset,  batch_size=args.per_device_eval_batch_size, drop_last = True) #sampler=valid_sampler,\n",
    "\n",
    "\n",
    "t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "optimizer = AdamW([\n",
    "            {'params': p_encoder.parameters()},\n",
    "            {'params': q_encoder.parameters()}\n",
    "        ], lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total)\n",
    "\n",
    "# Start training!\n",
    "global_step = 0\n",
    "best_acc = 0.0\n",
    "best_step = 0\n",
    "\n",
    "\n",
    "p_encoder.zero_grad()\n",
    "q_encoder.zero_grad()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "train_iterator = trange(int(args.num_train_epochs), desc=\"Epoch\")\n",
    "\n",
    "for _ in tqdm(train_iterator):\n",
    "    epoch_iterator = train_dataloader\n",
    "    train_losses = []\n",
    "    for step, batch in tqdm(enumerate(epoch_iterator)):\n",
    "      q_encoder.train()\n",
    "      p_encoder.train()\n",
    "\n",
    "      if torch.cuda.is_available():\n",
    "        batch = tuple(t.cuda() for t in batch)\n",
    "\n",
    "      p_inputs = {'input_ids': batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'token_type_ids': batch[2]\n",
    "                  }\n",
    "\n",
    "      q_inputs = {'input_ids': batch[3],\n",
    "                  'attention_mask': batch[4],\n",
    "                  'token_type_ids': batch[5]}\n",
    "\n",
    "      p_outputs = p_encoder(**p_inputs)  # (batch_size, emb_dim)\n",
    "      q_outputs = q_encoder(**q_inputs)  # (batch_size, emb_dim)\n",
    "\n",
    "\n",
    "      # Calculate similarity score & loss\n",
    "      sim_scores = torch.matmul(q_outputs, torch.transpose(p_outputs, 0, 1))  # (batch_size, emb_dim) x (emb_dim, batch_size) = (batch_size, batch_size)\n",
    "\n",
    "      # target: position of positive samples = diagonal element \n",
    "      targets = torch.arange(0, sim_scores.shape[0]).long()\n",
    "      if torch.cuda.is_available():\n",
    "        targets = targets.to('cuda')\n",
    "\n",
    "      sim_scores = F.log_softmax(sim_scores, dim=1)\n",
    "\n",
    "      loss = F.nll_loss(sim_scores, targets)\n",
    "\n",
    "\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      scheduler.step()\n",
    "      q_encoder.zero_grad()\n",
    "      p_encoder.zero_grad()\n",
    "      global_step += 1\n",
    "\n",
    "\n",
    "      if global_step % 5000 == 0 :\n",
    "        with torch.no_grad():\n",
    "          p_encoder.eval()\n",
    "          q_encoder.eval()\n",
    "\n",
    "          valid_loss = 0.0\n",
    "          valid_acc = 0.0\n",
    "          avg_valid_loss = 0.0\n",
    "          eval_correct = 0\n",
    "          eval_total = 0\n",
    "          for batch in tqdm(valid_loader):\n",
    "\n",
    "            batch = tuple(t.cuda() for t in batch)\n",
    "            p_inputs = {'input_ids': batch[0],\n",
    "                        'attention_mask': batch[1],\n",
    "                        'token_type_ids': batch[2]\n",
    "                        }\n",
    "\n",
    "            q_inputs = {'input_ids': batch[3],\n",
    "                        'attention_mask': batch[4],\n",
    "                        'token_type_ids': batch[5]\n",
    "                        }\n",
    "            p_outputs = p_encoder(**p_inputs)\n",
    "            q_outputs = q_encoder(**q_inputs)\n",
    "\n",
    "            sim_scores = torch.matmul(q_outputs, torch.transpose(p_outputs, 0, 1))\n",
    "            sim_scores = F.log_softmax(sim_scores, dim=1)\n",
    "            targets = torch.arange(0, sim_scores.shape[0]).long()\n",
    "            if torch.cuda.is_available():\n",
    "              targets = targets.to('cuda')\n",
    "\n",
    "            predict = torch.argmax(sim_scores, dim=1).long()\n",
    "            valid_loss= F.nll_loss(sim_scores, targets)\n",
    "\n",
    "            avg_valid_loss +=valid_loss.item()\n",
    "            eval_correct += (targets == predict).sum().item()\n",
    "            eval_total+=sim_scores.shape[0]\n",
    "\n",
    "          train_loss = loss.item()\n",
    "          valid_acc =  eval_correct/eval_total\n",
    "          avg_valid_loss = avg_valid_loss/eval_total\n",
    "\n",
    "          print( f\"train loss:{train_loss} eval acc:{valid_acc} avg_val_loss:{avg_valid_loss}\")\n",
    "          wandb.log({\"train_loss\":train_loss, 'eval acc': valid_acc, 'avg_val_loss': avg_valid_loss})\n",
    "\n",
    "          if valid_acc > best_acc:\n",
    "            best_acc = valid_acc\n",
    "            best_step = global_step\n",
    "            print(f\"best_acc: {best_acc} ,best_step_saved:{best_step}\")\n",
    "            wandb.log({\"best_acc\":best_acc, 'best_step_saved loss': best_step})\n",
    "\n",
    "            torch.save(p_encoder.state_dict(), f\"/opt/ml/input/dpr/best_p_model.pth\")   \n",
    "            torch.save(q_encoder.state_dict(), f\"/opt/ml/input/dpr/best_q_model.pth\")   \n",
    "\n",
    "      torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35848a2f-7598-4752-b0f3-d2e774b0677a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(p_encoder.state_dict(), f\"/opt/ml/input/dpr/best_p_model_last.pth\")   \n",
    "torch.save(q_encoder.state_dict(), f\"/opt/ml/input/dpr/best_q_model_last.pth\")   \n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0118b7bd-e9e8-4f5c-934c-2d01ac53a371",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
