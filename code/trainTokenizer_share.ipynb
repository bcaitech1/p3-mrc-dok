{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b99c9caf-6254-438e-b1a7-d561ee4deedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import numpy as np\n",
    "from datasets import load_from_disk\n",
    "from datasets import load_dataset\n",
    "\n",
    "import random\n",
    "import torch\n",
    "\n",
    "from konlpy.tag import Mecab\n",
    "mecab_tokenizer =Mecab().morphs\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83e2bfb8-2321-4573-a618-cb78b8ae3ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = load_from_disk('/opt/ml/code/data/train_dataset')\n",
    "training_dataset = datasets['train'][:]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcbc5b8e-f9bb-4cc2-813a-b91c8dc309d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_morph=[]\n",
    "for sentence in training_dataset['context']:\n",
    "    # 문장단위 mecab 적용\n",
    "    token_mecab_save= mecab_tokenizer(sentence)\n",
    "    total_morph.append(token_mecab_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88785243-c67d-4176-ad6c-4dc362fd584e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad_kor_v1 (/opt/ml/.cache/huggingface/datasets/squad_kor_v1/squad_kor_v1/1.0.0/31982418accc53b059af090befa81e68880acc667ca5405d30ce6fa7910950a7)\n"
     ]
    }
   ],
   "source": [
    "kodataset=load_dataset(\"squad_kor_v1\")\n",
    "ko_training_dataset = datasets['train'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5cb136e-4045-455c-a1eb-18a9f42d73b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in ko_training_dataset['context']:\n",
    "    # 문장단위 mecab 적용\n",
    "    token_mecab_save= mecab_tokenizer(sentence)\n",
    "    total_morph.append(token_mecab_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef6923df-5217-4455-98c5-7e19e0b21e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('after_mecab.txt', 'w', encoding='utf-8') as f:\n",
    "    for line in total_morph:\n",
    "        f.write(' '.join(line)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "632e58d8-c949-4571-ae64-80a0de33b1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1) define special tokens\n",
    "user_defined_symbols = ['[BOS]','[EOS]','[UNK0]','[UNK1]','[UNK2]','[UNK3]','[UNK4]','[UNK5]','[UNK6]','[UNK7]','[UNK8]','[UNK9]']\n",
    "unused_token_num = 200\n",
    "unused_list = ['[unused{}]'.format(n) for n in range(unused_token_num)]\n",
    "user_defined_symbols = user_defined_symbols + unused_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "898bfd9a-11eb-46ae-8842-d8a1d8b7458c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train complete\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tokenizers import BertWordPieceTokenizer, SentencePieceBPETokenizer, CharBPETokenizer, ByteLevelBPETokenizer\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer(strip_accents=False,  # Must be False if cased model\n",
    "                                   lowercase=False)\n",
    "\n",
    "corpus_file   = ['./after_mecab.txt']  # data path\n",
    "vocab_size    = 32000\n",
    "limit_alphabet= 6000\n",
    "output_path   = 'hugging_%d'%(vocab_size)\n",
    "min_frequency = 5\n",
    "\n",
    "\n",
    "# Then train it!\n",
    "tokenizer.train(files=corpus_file,\n",
    "               vocab_size=vocab_size,\n",
    "               min_frequency=min_frequency,\n",
    "               limit_alphabet=limit_alphabet,  \n",
    "               show_progress=True)\n",
    "print('train complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc60dbc4-bb8b-4b8c-94c9-9c211b31d039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tokenizer_model/vocab.txt']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save tokenizer\n",
    "hf_model_path='tokenizer_model'\n",
    "if not os.path.isdir(hf_model_path):\n",
    "    os.mkdir(hf_model_path)\n",
    "tokenizer.save_model(hf_model_path)  # vocab.txt 파일 한개가 만들어진다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b2bf0f8-43d5-43e3-b7ce-f164e745c673",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_for_load = BertTokenizer.from_pretrained(hf_model_path,\n",
    "                                                       strip_accents=False,  # Must be False if cased model\n",
    "                                                       lowercase=False)  # 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "371bbb9d-7f3d-4711-8c19-b49459014bf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# special token check\n",
    "tokenizer_for_load.all_special_tokens # 추가하기 전 기본적인 special token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "626c7866-bb9e-4086-9802-682d98b9190d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check special tokens : ['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]', '[BOS]', '[EOS]', '[UNK0]', '[UNK1]', '[UNK2]', '[UNK3]', '[UNK4]', '[UNK5]', '[UNK6]', '[UNK7]', '[UNK8]', '[UNK9]', '[unused0]', '[unused1]', '[unused2]']\n"
     ]
    }
   ],
   "source": [
    "# tokenizer에 special token 추가\n",
    "special_tokens_dict = {'additional_special_tokens': user_defined_symbols}\n",
    "tokenizer_for_load.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "# check tokenizer vocab with special tokens\n",
    "print('check special tokens : %s'%tokenizer_for_load.all_special_tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3df724f3-5933-4212-aaa6-26ece4f708fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tokenizer_model_special/tokenizer_config.json',\n",
       " 'tokenizer_model_special/special_tokens_map.json',\n",
       " 'tokenizer_model_special/vocab.txt',\n",
       " 'tokenizer_model_special/added_tokens.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save tokenizer model with special tokens\n",
    "tokenizer_for_load.save_pretrained(hf_model_path+'_special')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b513e5b5-4d13-4c2f-9521-f5e8eabcfe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(hf_model_path+'_special')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b60b2a-e642-4c94-b659-c9e86f649c40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
